{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dc99f2-2cef-48b8-86ed-d3d5bfab42e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 학습 시작 ---\n",
      "순환 가중치 Whh (초기 상태 일부):\n",
      "[[-0.01696312 -0.00205441]\n",
      " [ 0.00560655 -0.00546124]]\n",
      "\n",
      "Epoch 10/100, Loss: 0.3997\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[-0.01683986 -0.00204966]\n",
      " [ 0.00539843 -0.00541921]]\n",
      "\n",
      "Epoch 20/100, Loss: 0.3981\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[-0.01643088 -0.00177034]\n",
      " [ 0.00482824 -0.00577398]]\n",
      "\n",
      "Epoch 30/100, Loss: 0.3873\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[-0.01439533  0.00059348]\n",
      " [ 0.0019874  -0.00886318]]\n",
      "\n",
      "Epoch 40/100, Loss: 0.3296\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[-0.00315135  0.01775442]\n",
      " [-0.01356599 -0.02640534]]\n",
      "\n",
      "Epoch 50/100, Loss: 0.1830\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.03940565  0.10426338]\n",
      " [-0.07210296 -0.0615634 ]]\n",
      "\n",
      "Epoch 60/100, Loss: 0.1021\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.0728035   0.20560463]\n",
      " [-0.13044675 -0.07048593]]\n",
      "\n",
      "Epoch 70/100, Loss: 0.0729\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.09163396  0.23102933]\n",
      " [-0.15657206 -0.08272594]]\n",
      "\n",
      "Epoch 80/100, Loss: 0.0124\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.10846486  0.23818541]\n",
      " [-0.18741785 -0.09945152]]\n",
      "\n",
      "Epoch 90/100, Loss: 0.0001\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.10988219  0.23954865]\n",
      " [-0.20023074 -0.10430266]]\n",
      "\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "  순환 가중치 Whh (일부 업데이트):\n",
      "[[ 0.10969156  0.23977415]\n",
      " [-0.20231297 -0.10442212]]\n",
      "\n",
      "--- 학습 종료 ---\n",
      "순환 가중치 Whh (최종 상태 일부):\n",
      "[[ 0.10969156  0.23977415]\n",
      " [-0.20231297 -0.10442212]]\n",
      "\n",
      "--- 학습 후 간단 예측 테스트 ---\n",
      "입력 심볼 인덱스: 0, 모델이 예측한 다음 심볼 인덱스: 1 (실제 다음 심볼: 1)\n",
      "입력 심볼 인덱스: 1, 모델이 예측한 다음 심볼 인덱스: 2 (실제 다음 심볼: 2)\n",
      "입력 심볼 인덱스: 2, 모델이 예측한 다음 심볼 인덱스: 0 (실제 다음 심볼: 0)\n",
      "입력 심볼 인덱스: 3, 모델이 예측한 다음 심볼 인덱스: 4 (실제 다음 심볼: 4)\n",
      "입력 심볼 인덱스: 4, 모델이 예측한 다음 심볼 인덱스: 3 (실제 다음 심볼: 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. 활성화 함수 및 도함수 ---\n",
    "def tanh(x):\n",
    "    \"\"\"tanh 활성화 함수\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(tanh_x):\n",
    "    \"\"\"tanh 함수의 도함수. 입력은 이미 tanh(x)가 적용된 값이어야 함.\"\"\"\n",
    "    return 1 - tanh_x**2\n",
    "\n",
    "# --- 2. 손실 함수 (Mean Squared Error - MSE) ---\n",
    "def mse_loss(y_true, y_pred):\n",
    "    D = y_true.shape[1] # 출력 벡터의 차원 수\n",
    "    if D == 0:\n",
    "        return 0\n",
    "    squared_errors_sum = np.sum((y_pred - y_true)**2)\n",
    "    mse = squared_errors_sum / D\n",
    "    return 0.5 * mse # 0.5는 선택 사항\n",
    "\n",
    "# --- 3. SimpleRNN 직접 구현 클래스 ---\n",
    "class SimpleRNNNumpy:\n",
    "    def __init__(self, vocab_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size  # 입력 차원 (원-핫 벡터 크기)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size # 출력 차원 (예측하려는 원-핫 벡터 크기)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # 가중치 초기화 (작은 랜덤 값으로)\n",
    "        # 입력 -> 은닉층 가중치\n",
    "        self.Wxh = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        # 이전 은닉 상태 -> 현재 은닉 상태 가중치 (이것이 바로 '순환 가중치'입니다!)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        # 은닉층 -> 출력층 가중치\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        \n",
    "        # 편향 초기화 (0으로)\n",
    "        self.bh = np.zeros((1, hidden_size))  # 은닉층 편향\n",
    "        self.by = np.zeros((1, output_size))  # 출력층 편향\n",
    "\n",
    "        # 역전파 시 중간 계산값 저장을 위한 딕셔너리\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, inputs_sequence):\n",
    "        \"\"\"\n",
    "        순전파를 수행합니다.\n",
    "        inputs_sequence: 타임스텝별 입력 벡터의 리스트 (예: [[1,0,0], [0,1,0], ...])\n",
    "        \"\"\"\n",
    "        T = len(inputs_sequence)  # 시퀀스 길이\n",
    "        \n",
    "        # 중간값 저장을 위해 초기화\n",
    "        self.cache['x'] = {}  # 각 타임스텝의 입력\n",
    "        self.cache['h_linear'] = {} # 은닉층 활성화 전 값 (Wx*x + Wh*h_prev + b)\n",
    "        self.cache['h'] = {0: np.zeros((1, self.hidden_size))} # 초기 은닉 상태 h_0\n",
    "        self.cache['y_pred'] = {} # 각 타임스텝의 예측값\n",
    "\n",
    "        outputs_sequence_pred = [] # 최종 예측값들을 저장할 리스트\n",
    "\n",
    "        for t in range(T):\n",
    "            xt = np.array(inputs_sequence[t]).reshape(1, -1) # 현재 타임스텝 입력 (1, vocab_size)\n",
    "            ht_prev = self.cache['h'][t] # 이전 타임스텝의 은닉 상태\n",
    "\n",
    "            self.cache['x'][t] = xt\n",
    "\n",
    "            # 은닉 상태 계산: h_t = tanh(x_t @ Wxh + h_{t-1} @ Whh + b_h)\n",
    "            ht_linear = np.dot(xt, self.Wxh) + np.dot(ht_prev, self.Whh) + self.bh\n",
    "            ht = tanh(ht_linear)\n",
    "            \n",
    "            self.cache['h_linear'][t] = ht_linear\n",
    "            self.cache['h'][t+1] = ht # 다음 계산을 위해 저장 (t+1 인덱스 사용 주의)\n",
    "\n",
    "            # 출력 계산: y_t = h_t @ Why + b_y (여기서는 활성화 함수 없이 선형 출력)\n",
    "            yt_pred = np.dot(ht, self.Why) + self.by\n",
    "            self.cache['y_pred'][t] = yt_pred\n",
    "            outputs_sequence_pred.append(yt_pred)\n",
    "            \n",
    "        return outputs_sequence_pred\n",
    "\n",
    "    def backward(self, inputs_sequence, targets_sequence, outputs_sequence_pred):\n",
    "        \"\"\"\n",
    "        역전파 (BPTT - Backpropagation Through Time)를 수행합니다.\n",
    "        \"\"\"\n",
    "        T = len(inputs_sequence)\n",
    "        \n",
    "        # 그래디언트 변수들 초기화 (가중치와 동일한 크기로)\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        # 다음 타임스텝으로부터 전달될 은닉 상태의 그래디언트 (초기에는 0)\n",
    "        dh_next = np.zeros((1, self.hidden_size)) \n",
    "        \n",
    "        total_loss_for_sequence = 0\n",
    "\n",
    "        # 시퀀스의 마지막부터 처음까지 역방향으로 순회\n",
    "        for t in reversed(range(T)):\n",
    "            xt = self.cache['x'][t]\n",
    "            ht = self.cache['h'][t+1]       # 순전파 시 t+1 인덱스에 저장된 h_t\n",
    "            ht_prev = self.cache['h'][t] # h_{t-1}\n",
    "            # ht_linear = self.cache['h_linear'][t] # h_t 계산 시 tanh 이전 값\n",
    "\n",
    "            yt_pred = self.cache['y_pred'][t]\n",
    "            yt_true = np.array(targets_sequence[t]).reshape(1, -1)\n",
    "\n",
    "            # 현재 타임스텝의 손실 계산 (정보용)\n",
    "            loss_t = mse_loss(yt_true, yt_pred)\n",
    "            total_loss_for_sequence += loss_t\n",
    "            \n",
    "            # --- 출력층의 그래디언트 계산 ---\n",
    "            # 손실 함수(MSE)의 예측값 yt_pred에 대한 도함수: (yt_pred - yt_true)\n",
    "            # (mse_loss에서 0.5를 곱했으므로, 미분 시 0.5 * 2 * (y_pred - y_true) = (y_pred - y_true))\n",
    "            # 평균을 취했으므로, yt_true.shape[0]로 나눠줌 (여기서는 1)\n",
    "            dy_pred = (yt_pred - yt_true) / yt_true.shape[0]\n",
    "\n",
    "            # Why와 by에 대한 그래디언트 누적\n",
    "            dWhy += np.dot(ht.T, dy_pred)\n",
    "            dby += dy_pred\n",
    "            \n",
    "            # --- 은닉층의 그래디언트 계산 (BPTT의 핵심) ---\n",
    "            # 현재 은닉 상태 ht에 대한 그래디언트\n",
    "            # 1. 출력층으로부터 오는 그래디언트: dy_pred @ Why.T\n",
    "            # 2. 다음 타임스텝 t+1의 은닉 상태로부터 오는 그래디언트: dh_next\n",
    "            dh = np.dot(dy_pred, self.Why.T) + dh_next\n",
    "            \n",
    "            # tanh 활성화 함수의 그래디언트 적용\n",
    "            # d(loss)/d(ht_linear) = d(loss)/d(ht) * d(ht)/d(ht_linear)\n",
    "            # d(ht)/d(ht_linear)는 dtanh(ht) 즉, (1 - ht**2)\n",
    "            dh_raw = dh * dtanh(ht) # dtanh는 이미 tanh가 적용된 ht를 인자로 받음\n",
    "            \n",
    "            # bh에 대한 그래디언트 누적\n",
    "            dbh += dh_raw\n",
    "            \n",
    "            # Wxh에 대한 그래디언트 누적\n",
    "            dWxh += np.dot(xt.T, dh_raw)\n",
    "            \n",
    "            # Whh (순환 가중치)에 대한 그래디언트 누적! 이것이 핵심입니다.\n",
    "            dWhh += np.dot(ht_prev.T, dh_raw)\n",
    "            \n",
    "            # 다음 반복(t-1)을 위해, 현재 dh_raw가 이전 은닉 상태 ht_prev에 미치는 영향(dh_next_for_prev_step)을 계산\n",
    "            dh_next = np.dot(dh_raw, self.Whh.T)\n",
    "            \n",
    "        # (선택적) 그래디언트 클리핑: 그래디언트 폭주를 막기 위해 사용\n",
    "        # 여기서는 매우 간단한 형태로 구현하거나, 값의 범위를 보고 생략 가능\n",
    "        clip_value = 5.0\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -clip_value, clip_value, out=dparam)\n",
    "            \n",
    "        return total_loss_for_sequence, dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def update_weights(self, dWxh, dWhh, dWhy, dbh, dby):\n",
    "        \"\"\"단순 경사 하강법으로 가중치를 업데이트합니다.\"\"\"\n",
    "        self.Wxh -= self.learning_rate * dWxh\n",
    "        self.Whh -= self.learning_rate * dWhh # 순환 가중치 업데이트!\n",
    "        self.Why -= self.learning_rate * dWhy\n",
    "        self.bh  -= self.learning_rate * dbh\n",
    "        self.by  -= self.learning_rate * dby\n",
    "\n",
    "# --- 4. 더미 데이터 생성 및 학습 루프 ---\n",
    "\n",
    "# 하이퍼파라미터\n",
    "vocab_size = 5    # 입력/출력 단어(심볼)의 종류 수\n",
    "hidden_size = 4   # 은닉 상태의 크기 (자유롭게 설정)\n",
    "output_size = vocab_size # 다음 심볼을 예측하므로 vocab_size와 동일\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# 더미 시퀀스 데이터: 0 -> 1 -> 2 -> 0 (순환)\n",
    "# 입력: [0, 1, 2], 타겟: [1, 2, 0] (다음 스텝 예측)\n",
    "# 원-핫 인코딩으로 표현\n",
    "# x_0 = [1,0,0], x_1 = [0,1,0], x_2 = [0,0,1]\n",
    "# y_0_true = [0,1,0], y_1_true = [0,0,1], y_2_true = [1,0,0]\n",
    "\n",
    "inputs_indices = [0, 1, 2, 3, 4]\n",
    "targets_indices = [1, 2, 0, 4, 3] \n",
    "\n",
    "# 원-핫 인코딩된 시퀀스 생성\n",
    "inputs_sequence_onehot = [np.eye(vocab_size)[i] for i in inputs_indices]\n",
    "targets_sequence_onehot = [np.eye(vocab_size)[i] for i in targets_indices]\n",
    "\n",
    "# RNN 모델 인스턴스 생성\n",
    "rnn = SimpleRNNNumpy(vocab_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "print(\"--- 학습 시작 ---\")\n",
    "print(f\"순환 가중치 Whh (초기 상태 일부):\\n{rnn.Whh[:2, :2]}\\n\") # Whh의 일부 값만 출력\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    # 1. 순전파\n",
    "    predictions_sequence = rnn.forward(inputs_sequence_onehot)\n",
    "    \n",
    "    # 2. 역전파 (BPTT)\n",
    "    total_loss, dWxh, dWhh, dWhy, dbh, dby = rnn.backward(inputs_sequence_onehot, targets_sequence_onehot, predictions_sequence)\n",
    "    \n",
    "    # 3. 가중치 업데이트\n",
    "    rnn.update_weights(dWxh, dWhh, dWhy, dbh, dby)\n",
    "    \n",
    "    # 10 에포크마다 손실 및 Whh의 변화 출력\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "        print(f\"  순환 가중치 Whh (일부 업데이트):\\n{rnn.Whh[:2, :2]}\\n\")\n",
    "\n",
    "print(\"--- 학습 종료 ---\")\n",
    "print(f\"순환 가중치 Whh (최종 상태 일부):\\n{rnn.Whh[:2, :2]}\")\n",
    "\n",
    "# 학습 후 예측 테스트\n",
    "print(\"\\n--- 학습 후 간단 예측 테스트 ---\")\n",
    "# 테스트 시에는 이전 은닉 상태를 수동으로 관리해야 합니다.\n",
    "current_h_state = np.zeros((1, hidden_size)) # 초기 은닉 상태\n",
    "for i in range(len(inputs_sequence_onehot)):\n",
    "    xt_test = np.array(inputs_sequence_onehot[i]).reshape(1, -1)\n",
    "    ht_linear_test = np.dot(xt_test, rnn.Wxh) + np.dot(current_h_state, rnn.Whh) + rnn.bh\n",
    "    ht_test = tanh(ht_linear_test) # 현재 은닉 상태\n",
    "    yt_pred_test = np.dot(ht_test, rnn.Why) + rnn.by # 현재 예측\n",
    "    \n",
    "    predicted_next_symbol_index = np.argmax(yt_pred_test) # 가장 확률 높은 다음 심볼 인덱스\n",
    "    \n",
    "    print(f\"입력 심볼 인덱스: {inputs_indices[i]}, \"\n",
    "          f\"모델이 예측한 다음 심볼 인덱스: {predicted_next_symbol_index} (실제 다음 심볼: {targets_indices[i]})\")\n",
    "    \n",
    "    current_h_state = ht_test # 다음 예측을 위해 현재 은닉 상태를 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e109e6-2b6c-4a18-8717-1a0957d88c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "crawl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
